{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#IMPORT THE LIBRARIES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# librosa is a Python library for analyzing audio and music. It can be used to extract the data from the audio files we will see it later.\n",
    "import librosa\n",
    "import librosa.display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# to play the audio files\n",
    "import IPython.display as ipd\n",
    "from IPython.display import Audio\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM,BatchNormalization , GRU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.layers import Input, Flatten, Dropout, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "import tensorflow as tf \n",
    "print (\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "Tess = \"tess toronto emotional speech set data/TESS Toronto emotional speech set data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TESS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tess_directory_list = os.listdir(Tess)\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "\n",
    "for dir in tess_directory_list:\n",
    "    directories = os.listdir(Tess + dir)\n",
    "    for file in directories:\n",
    "        part = file.split('.')[0]\n",
    "        part = part.split('_')[2]\n",
    "        if part=='ps':\n",
    "            file_emotion.append('surprise')\n",
    "        else:\n",
    "            file_emotion.append(part)\n",
    "        file_path.append(Tess + dir + '/' + file)\n",
    "        \n",
    "# dataframe for emotion of files\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "# dataframe for path of files.\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "Tess_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "Tess_df.head()\n",
    "print(Tess_df.Emotions.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Integration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# creating Dataframe using all the 4 dataframes we created so far.\n",
    "data_path = Tess_df\n",
    "data_path.to_csv(\"data_path.csv\",index=False)\n",
    "data_path.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(data_path.Emotions.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*                           Data Visualisation and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.title('Count of Emotions', size=16)\n",
    "sns.countplot(data_path.Emotions)\n",
    "plt.ylabel('Count', size=12)\n",
    "plt.xlabel('Emotions', size=12)\n",
    "sns.despine(top=True, right=True, left=False, bottom=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data,sr = librosa.load(file_path[0])\n",
    "sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ipd.Audio(data,rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "mfcc = librosa.feature.mfcc(y=data, sr=sr, n_mfcc=30)\n",
    "chroma = librosa.feature.chroma_stft(y=data, sr=sr)\n",
    "\n",
    "# MFCC\n",
    "plt.figure(figsize=(16, 10))\n",
    "plt.subplot(3,1,1)\n",
    "librosa.display.specshow(mfcc, x_axis='time')\n",
    "plt.ylabel('MFCC')\n",
    "plt.colorbar()\n",
    "\n",
    "# Chroma\n",
    "plt.subplot(3,1,2)\n",
    "librosa.display.specshow(chroma, x_axis='time')\n",
    "plt.ylabel('Chroma')\n",
    "plt.colorbar()\n",
    "\n",
    "ipd.Audio(data,rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split BEFORE Augmentation\n",
    "\n",
    "**Important: We split the data first, then apply augmentation only to training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X_train_paths, X_test_paths, y_train, y_test = train_test_split(\n",
    "    data_path['Path'], \n",
    "    data_path['Emotions'], \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=data_path['Emotions']\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train_paths)}\")\n",
    "print(f\"Test set size: {len(X_test_paths)}\")\n",
    "print(f\"Training emotion distribution:\\n{y_train.value_counts()}\")\n",
    "print(f\"Test emotion distribution:\\n{y_test.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# NOISE\n",
    "def noise(data):\n",
    "    noise_amp = 0.035*np.random.uniform()*np.amax(data)\n",
    "    data = data + noise_amp*np.random.normal(size=data.shape[0])\n",
    "    return data\n",
    "    \n",
    "# PITCH\n",
    "def pitch(data, sampling_rate, pitch_factor=0.7):\n",
    "    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)\n",
    "\n",
    "# STRETCH (time stretching)\n",
    "def stretch(data, rate=0.8):\n",
    "    return librosa.effects.time_stretch(data, rate)\n",
    "\n",
    "# SHIFT (time shifting)\n",
    "def shift(data):\n",
    "    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n",
    "    return np.roll(data, shift_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# NORMAL AUDIO\n",
    "\n",
    "import librosa.display\n",
    "plt.figure(figsize=(12, 5))\n",
    "librosa.display.waveshow(y=data, sr=sr)\n",
    "ipd.Audio(data,rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# AUDIO WITH NOISE\n",
    "x = noise(data)\n",
    "plt.figure(figsize=(12,5))\n",
    "librosa.display.waveshow(y=x, sr=sr)\n",
    "ipd.Audio(x, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# STRETCHED AUDIO\n",
    "x = stretch(data)\n",
    "plt.figure(figsize=(12, 5))\n",
    "librosa.display.waveshow(y=x, sr=sr)\n",
    "ipd.Audio(x, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# SHIFTED AUDIO\n",
    "x = shift(data)\n",
    "plt.figure(figsize=(12,5))\n",
    "librosa.display.waveshow(y=x, sr=sr)\n",
    "ipd.Audio(x, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# AUDIO WITH PITCH\n",
    "x = pitch(data, sr)\n",
    "plt.figure(figsize=(12, 5))\n",
    "librosa.display.waveshow(y=x, sr=sr)\n",
    "ipd.Audio(x, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction functions\n",
    "\n",
    "**Combined MFCC and Chroma feature extraction for 1D CNN + Attention architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def zcr(data,frame_length,hop_length):\n",
    "    zcr=librosa.feature.zero_crossing_rate(data,frame_length=frame_length,hop_length=hop_length)\n",
    "    return np.squeeze(zcr)\n",
    "\n",
    "def rmse(data,frame_length=2048,hop_length=512):\n",
    "    rmse=librosa.feature.rms(data,frame_length=frame_length,hop_length=hop_length)\n",
    "    return np.squeeze(rmse)\n",
    "\n",
    "def mfcc(data,sr,frame_length=2048,hop_length=512,flatten:bool=True):\n",
    "    mfcc=librosa.feature.mfcc(data,sr=sr)\n",
    "    return np.squeeze(mfcc.T)if not flatten else np.ravel(mfcc.T)\n",
    "\n",
    "def calculate_chroma(data, sr, hop_length=512, flatten=True):\n",
    "    chroma = librosa.feature.chroma_stft(y=data, sr=sr, hop_length=hop_length)\n",
    "    return np.squeeze(chroma.T) if not flatten else np.ravel(chroma.T)\n",
    "\n",
    "def extract_features(data,sr=22050,frame_length=2048,hop_length=512):\n",
    "    result=np.array([])\n",
    "    \n",
    "    # Combine MFCC and Chroma features\n",
    "    result=np.hstack((result,\n",
    "                      mfcc(data,sr,frame_length,hop_length),\n",
    "                      calculate_chroma(data, sr, hop_length)\n",
    "                     ))\n",
    "    return result\n",
    "\n",
    "def get_features_with_augmentation(path,duration=2.5, offset=0.6):\n",
    "    \"\"\"Extract features with augmentation - only for training data\"\"\"\n",
    "    data,sr=librosa.load(path,duration=duration,offset=offset)\n",
    "    aud=extract_features(data)\n",
    "    audio=np.array(aud)\n",
    "    \n",
    "    # Apply different augmentations\n",
    "    noised_audio=noise(data)\n",
    "    aud2=extract_features(noised_audio)\n",
    "    audio=np.vstack((audio,aud2))\n",
    "    \n",
    "    pitched_audio=pitch(data,sr)\n",
    "    aud3=extract_features(pitched_audio)\n",
    "    audio=np.vstack((audio,aud3))\n",
    "    \n",
    "    stretched_audio=stretch(data)\n",
    "    aud4=extract_features(stretched_audio)\n",
    "    audio=np.vstack((audio,aud4))\n",
    "    \n",
    "    shifted_audio=shift(data)\n",
    "    aud5=extract_features(shifted_audio)\n",
    "    audio=np.vstack((audio,aud5))\n",
    "    \n",
    "    # Combined augmentations\n",
    "    pitched_noised_audio=noise(pitch(data,sr))\n",
    "    aud6=extract_features(pitched_noised_audio)\n",
    "    audio=np.vstack((audio,aud6))\n",
    "    \n",
    "    return audio\n",
    "\n",
    "def get_features_no_augmentation(path,duration=2.5, offset=0.6):\n",
    "    \"\"\"Extract features without augmentation - for test data\"\"\"\n",
    "    data,sr=librosa.load(path,duration=duration,offset=offset)\n",
    "    aud=extract_features(data)\n",
    "    return np.array(aud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "print(\"Number of processors: \", mp.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction for Training Data (WITH augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Processing training data with augmentation...\")\n",
    "start = timeit.default_timer()\n",
    "X_train_features, y_train_extended = [], []\n",
    "\n",
    "for path, emotion, index in tqdm(zip(X_train_paths, y_train, range(len(X_train_paths)))):\n",
    "    features = get_features_with_augmentation(path)\n",
    "    if index % 500 == 0:\n",
    "        print(f'{index} training audio files have been processed')\n",
    "    \n",
    "    # Each audio file generates multiple augmented versions\n",
    "    for i in features:\n",
    "        X_train_features.append(i)\n",
    "        y_train_extended.append(emotion)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print(f'Training data processing completed in {stop - start:.2f} seconds')\n",
    "print(f'Training features shape: {len(X_train_features)} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction for Test Data (WITHOUT augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Processing test data without augmentation...\")\n",
    "start = timeit.default_timer()\n",
    "X_test_features, y_test_list = [], []\n",
    "\n",
    "for path, emotion, index in tqdm(zip(X_test_paths, y_test, range(len(X_test_paths)))):\n",
    "    features = get_features_no_augmentation(path)\n",
    "    if index % 100 == 0:\n",
    "        print(f'{index} test audio files have been processed')\n",
    "    \n",
    "    # Test data: only original features, no augmentation\n",
    "    X_test_features.append(features)\n",
    "    y_test_list.append(emotion)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print(f'Test data processing completed in {stop - start:.2f} seconds')\n",
    "print(f'Test features shape: {len(X_test_features)} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"Original data size: {len(data_path)}\")\n",
    "print(f\"Training set size (before augmentation): {len(X_train_paths)}\")\n",
    "print(f\"Training set size (after augmentation): {len(X_train_features)}\")\n",
    "print(f\"Test set size (no augmentation): {len(X_test_features)}\")\n",
    "print(f\"Augmentation factor for training: {len(X_train_features) / len(X_train_paths):.1f}x\")\n",
    "\n",
    "# Check feature dimensions\n",
    "if len(X_train_features) > 0:\n",
    "    print(f\"Feature vector dimension: {len(X_train_features[0])}\")\n",
    "\n",
    "# Check emotion distribution in augmented training set\n",
    "train_emotion_counts = pd.Series(y_train_extended).value_counts()\n",
    "print(f\"\\nTraining emotion distribution (after augmentation):\\n{train_emotion_counts}\")\n",
    "test_emotion_counts = pd.Series(y_test_list).value_counts()\n",
    "print(f\"\\nTest emotion distribution:\\n{test_emotion_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving processed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save training data\n",
    "Train_features = pd.DataFrame(X_train_features)\n",
    "Train_features['Emotions'] = y_train_extended\n",
    "Train_features.to_csv('train_features_mfcc_chroma_aug.csv', index=False)\n",
    "print(\"Training features saved to 'train_features_mfcc_chroma_aug.csv'\")\n",
    "\n",
    "# Save test data\n",
    "Test_features = pd.DataFrame(X_test_features)\n",
    "Test_features['Emotions'] = y_test_list\n",
    "Test_features.to_csv('test_features_mfcc_chroma_no_aug.csv', index=False)\n",
    "print(\"Test features saved to 'test_features_mfcc_chroma_no_aug.csv'\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nTraining features preview:\")\n",
    "print(Train_features.head())\n",
    "print(\"\\nTest features preview:\")\n",
    "print(Test_features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading saved features (for subsequent use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Example of loading the processed features for model training\n",
    "# Train_features = pd.read_csv('train_features_mfcc_chroma_aug.csv')\n",
    "# Test_features = pd.read_csv('test_features_mfcc_chroma_no_aug.csv')\n",
    "# print(\"Features loaded successfully!\")\n",
    "# print(f\"Training data shape: {Train_features.shape}\")\n",
    "# print(f\"Test data shape: {Test_features.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 107620,
     "sourceId": 256618,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 316368,
     "sourceId": 639622,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 325566,
     "sourceId": 653195,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 338555,
     "sourceId": 671851,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3468263,
     "sourceId": 6060815,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30380,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ser",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}